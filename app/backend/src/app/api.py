"""Minimal FastAPI relay for chatting with an OpenAI GPT-5 model.

This module exposes a single POST endpoint `/chat` that accepts a user message
(and optional prior turns) and returns the model's reply using OpenAI's
Responses API.

Environment:
    OPENAI_API_KEY: Your OpenAI API key.

Run:
    uvicorn app:app --host 0.0.0.0 --port 8000 --reload
"""

from __future__ import annotations

import os
from typing import List, Literal, Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field, field_validator
from openai import OpenAI
from openai.types.responses import Response


# --- Pydantic request/response models -----------------------------------------------------------


class ChatTurn(BaseModel):
    """A single conversational turn.

    Attributes:
        role: The speaker role. Must be one of "system", "user", or "assistant".
        content: Plain text content for the turn.
    """

    role: Literal["system", "user", "assistant"]
    content: str = Field(..., min_length=1)

    @field_validator("content")
    @classmethod
    def _strip(cls, v: str) -> str:
        """Trim whitespace and validate non-empty content.

        Args:
            v: Raw content string.

        Returns:
            The stripped content string.

        Raises:
            ValueError: If the stripped content is empty.
        """
        s = v.strip()
        if not s:
            raise ValueError("content cannot be empty/whitespace")
        return s


class ChatRequest(BaseModel):
    """Request body for the /chat endpoint.

    Attributes:
        message: The current user message to send to the model.
        history: Optional prior conversation turns that you want to include in
            context (system/user/assistant). If omitted, a single-turn request
            is made.
        model: The OpenAI model identifier to use. Defaults to a GPT-5 model.
    """

    message: str = Field(..., min_length=1)
    history: Optional[List[ChatTurn]] = None
    model: str = Field(
        default="gpt-5",
        description=(
            "OpenAI model name. Keep default unless you know you have access to a specific GPT-5 family variant."
        ),
    )


class ChatResponse(BaseModel):
    """Response body for the /chat endpoint.

    Attributes:
        model: The model that generated the response.
        output_text: Convenience field with the concatenated text output.
        raw_id: The OpenAI response object id (useful for observability).
    """

    model: str
    output_text: str
    raw_id: str


# --- Service setup ------------------------------------------------------------------------------


def _build_openai_client() -> OpenAI:
    """Create the OpenAI client using the environment API key.

    Returns:
        An initialized OpenAI client.

    Raises:
        RuntimeError: If the API key is missing.
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise RuntimeError("OPENAI_API_KEY is not set. Export it in your environment.")
    return OpenAI()


app = FastAPI(title="GPT-5 Chat Relay", version="1.0.0")
_client = _build_openai_client()


# --- Helpers ------------------------------------------------------------------------------------


def _to_responses_api_input(req: ChatRequest) -> list:
    """Convert a ChatRequest to the Responses API `input` format.

    The Responses API accepts an `input` payload that can be a list of messages,
    where each message has a `role` and a list of typed content parts. For text,
    we use `type: "input_text"` as shown in OpenAI quickstart examples.

    Args:
        req: Validated ChatRequest.

    Returns:
        A list appropriate for `client.responses.create(input=...)`.
    """
    messages = []
    if req.history:
        for turn in req.history:
            messages.append({
                "role": turn.role,
                "content": [{"type": "input_text", "text": turn.content}],
            })

    # Append the current user message as the final turn.
    messages.append({"role": "user", "content": [{"type": "input_text", "text": req.message}]})
    return messages


def _extract_output_text(response: Response) -> str:
    """Extract a plain text reply from a Responses API object.

    The Responses API can return multiple content items; `output_text` is a
    convenience property exposed by the SDK that concatenates text outputs.

    Args:
        response: The OpenAI Response object.

    Returns:
        The concatenated text output generated by the model.
    """
    # The Python SDK exposes .output_text; fall back to a safe default.
    return getattr(response, "output_text", "") or ""


# --- API route ----------------------------------------------------------------------------------


@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest) -> ChatResponse:
    """Relay a message to OpenAI GPT-5 and return the model's reply.

    This endpoint constructs a Responses API `input` from the provided
    history + message, calls the OpenAI API, and returns a minimal JSON
    payload containing the model and text output.

    Args:
        req: The validated chat request body.

    Returns:
        A `ChatResponse` containing the model name, the final text output, and
        the raw OpenAI response id.

    Raises:
        HTTPException: If the upstream OpenAI call fails or returns no text.
    """
    try:
        responses_input = _to_responses_api_input(req)
        resp: Response = _client.responses.create(
            model=req.model,
            input=responses_input,
            # You can set timeouts or temperature here, e.g.:
            # temperature=0.7,
            # max_output_tokens=4096,
        )
        text = _extract_output_text(resp)
        if not text:
            raise RuntimeError("OpenAI returned no text output.")
        return ChatResponse(model=req.model, output_text=text, raw_id=resp.id)
    except Exception as exc:  # Broad by design to surface clear HTTP errors.
        raise HTTPException(status_code=502, detail=str(exc)) from exc
